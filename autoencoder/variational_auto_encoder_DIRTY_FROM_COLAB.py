# -*- coding: utf-8 -*-
"""variational_auto_encoder

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B-__4p5fIfl5Xmqltj2z4VhKFAjTmIR_
"""

!pip install kagglehub

import kagglehub

# Download latest version
path = kagglehub.dataset_download("chrisfilo/fruit-recognition")

print("Path to dataset files:", path)

import os
import cv2
import numpy as np
import random


def load_images_from_folder(folder, target_size=(32, 32), max_images_per_class=200, n_classes=5):
    """
    Load images from folder structure, handling both nested and direct image folders.
    Randomly samples n_classes from available classes.
    """
    images = []
    labels = []

    # First, collect all available classes
    all_classes = []

    for item in os.listdir(folder):
        item_path = os.path.join(folder, item)
        if os.path.isdir(item_path):
            # Check if this folder has subfolders (like Apple/Apple A) or direct images
            subitems = os.listdir(item_path)
            has_subfolders = any(os.path.isdir(os.path.join(item_path, subitem)) for subitem in subitems)

            if has_subfolders:
                # Handle nested structure (like Apple/Apple A, Apple/Apple B)
                for subfolder in subitems:
                    subfolder_path = os.path.join(item_path, subfolder)
                    if os.path.isdir(subfolder_path):
                        # Check if this subfolder contains images
                        image_files = [f for f in os.listdir(subfolder_path)
                                       if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                        if image_files:
                            all_classes.append((subfolder_path, f"{item}_{subfolder}"))
            else:
                # Handle direct image structure (like Carambola with images directly)
                image_files = [f for f in os.listdir(item_path)
                               if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                if image_files:
                    all_classes.append((item_path, item))

    # Randomly sample n_classes
    if len(all_classes) > n_classes:
        selected_classes = random.sample(all_classes, n_classes)
    else:
        selected_classes = all_classes
        print(f"Warning: Only {len(all_classes)} classes available, using all of them.")

    print(f"Selected classes: {[class_name for _, class_name in selected_classes]}")

    # Load images from selected classes
    for class_folder, class_name in selected_classes:
        image_files = [f for f in os.listdir(class_folder)
                       if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

        # Randomly sample images if there are more than max_images_per_class
        if len(image_files) > max_images_per_class:
            selected_files = random.sample(image_files, max_images_per_class)
        else:
            selected_files = image_files

        print(f"Loading {len(selected_files)} images from {class_name}")

        for filename in selected_files:
            img_path = os.path.join(class_folder, filename)
            img = cv2.imread(img_path)
            if img is not None:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = cv2.resize(img, target_size)
                images.append(img)
                labels.append(class_name)

    print(f"Total images loaded: {len(images)}")
    return images, labels


def normalize_images(images):
    return [img / 255.0 for img in images]


def load_and_normalize_images(images, flatten=True):
    normalized_images = []
    for img in images:
        normalized_image = img.astype(np.float32) / 255.0
        if flatten:
            normalized_image = normalized_image.flatten()
        normalized_images.append(normalized_image)

    return normalized_images

import numpy as np
import pandas as pd

class DataNormalizer:
    def __init__(self, method=None):
        self.method = method
        self.min_val = None
        self.max_val = None
        self.is_fitted = False

    def _to_numpy(self, data):
        if isinstance(data, (pd.DataFrame, pd.Series)):
            return data.values
        elif hasattr(data, 'values'):
            return data.values
        else:
            return np.array(data)

    def fit(self, data):
        data_array = self._to_numpy(data)

        self.min_val = data_array.min()
        self.max_val = data_array.max()
        self.is_fitted = True
        return self

    def transform(self, data):
        if not self.is_fitted:
            raise ValueError("Normalizer must be fitted before transforming data")

        data_array = self._to_numpy(data)

        if self.method == "tanh":
            normalized = (data_array - self.min_val) / (self.max_val - self.min_val)
            return 2 * normalized - 1
        else:
            return (data_array - self.min_val) / (self.max_val - self.min_val)


    def fit_transform(self, data):
        return self.fit(data).transform(data)

    def inverse_transform(self, data):
        if not self.is_fitted:
            raise ValueError("Normalizer must be fitted before inverse transforming data")

        data_array = self._to_numpy(data)

        if self.method == "tanh":
            normalized = (data_array + 1) / 2
            return normalized * (self.max_val - self.min_val) + self.min_val
        else:
            return data_array * (self.max_val - self.min_val) + self.min_val

import torch
import numpy as np


def encode_dataset(model, data, batch_size=1000, use_mean=True):
    device = next(model.parameters()).device
    model.eval()

    latents = []

    with torch.no_grad():
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            batch_tensor = torch.FloatTensor(batch).to(device)

            if use_mean:
                mu, logvar = model.encode_to_params(batch_tensor)
                encoded = mu
            else:
                encoded = model.encode(batch_tensor)

            latents.append(encoded.cpu().numpy())

    model.train()
    return np.vstack(latents)


def generate_samples(model, latents, n_samples=100):
    device = next(model.parameters()).device
    model.eval()

    latent_mean = np.mean(latents, axis=0)
    latent_std = np.std(latents, axis=0)

    generated_samples = []

    with torch.no_grad():
        for _ in range(n_samples):
            random_latent = np.random.normal(latent_mean, latent_std, size=(1, latents.shape[1]))
            latent_tensor = torch.FloatTensor(random_latent).to(device)

            decoded = model.decode(latent_tensor)
            generated_samples.append(decoded.cpu().numpy())

    model.train()
    return np.vstack(generated_samples)

from torch import nn
import torch


class ConvAutoencoder(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()
        input_shape = kwargs["input_shape"]  # Now expects 3072 (32*32*3)

        activations = {
            None: nn.ReLU(),
            "relu": nn.ReLU(),
            "tanh": nn.Tanh(),
            "sigmoid": nn.Sigmoid(),
            "leaky_relu": nn.LeakyReLU(0.2)
        }

        self.activation_function = activations[kwargs.get("activation_function", "relu")]

        if kwargs.get("activation_function") == "tanh":
            final_activation = nn.Tanh()
        else:
            final_activation = nn.Sigmoid()

        # Encoder: 32x32x3 -> 2x2x128 -> 2D latent space
        self.encoder = nn.Sequential(
            # Input: 3x32x32
            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # -> 32x16x16
            nn.BatchNorm2d(32),
            self.activation_function,

            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # -> 64x8x8
            nn.BatchNorm2d(64),
            self.activation_function,

            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # -> 128x4x4
            nn.BatchNorm2d(128),
            self.activation_function,

            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),  # -> 128x2x2
            nn.BatchNorm2d(128),
            self.activation_function,

            nn.Flatten(),  # -> 512
            nn.Linear(512, 256),
            self.activation_function,
            nn.Linear(256, 2)  # 2D latent space
        )

        # Decoder: 2D latent space -> 2x2x128 -> 32x32x3
        self.decoder = nn.Sequential(
            nn.Linear(2, 256),
            self.activation_function,
            nn.Linear(256, 512),
            self.activation_function,

            nn.Unflatten(1, (128, 2, 2)),  # -> 128x2x2

            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # -> 128x4x4
            nn.BatchNorm2d(128),
            self.activation_function,

            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # -> 64x8x8
            nn.BatchNorm2d(64),
            self.activation_function,

            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, output_padding=0),  # -> 32x16x16
            nn.BatchNorm2d(32),
            self.activation_function,

            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1, output_padding=0),  # -> 3x32x32
            final_activation
        )

    def encode(self, x):
        batch_size = x.size(0)
        x = x.view(batch_size, 3, 32, 32)  # Reshape to 3x32x32
        return self.encoder(x)

    def decode(self, z):
        decoded = self.decoder(z)
        batch_size = decoded.size(0)
        return decoded.view(batch_size, 3072)  # Flatten to 3072 (32*32*3)

    def forward(self, x):
        batch_size = x.size(0)
        x = x.view(batch_size, 3, 32, 32)  # Reshape to 3x32x32
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        decoded = decoded.view(batch_size, 3072)  # Return flattened output
        return decoded

    def reconstruct(self, x):
        """Added for compatibility with VAE interface"""
        return self.forward(x)

import torch
import numpy as np
from torch.utils.tensorboard import SummaryWriter


def create_batches(data, batch_size, shuffle=True):
    n_samples = len(data)
    indices = np.arange(n_samples)

    if shuffle:
        np.random.shuffle(indices)

    batches = []
    for start_idx in range(0, n_samples, batch_size):
        end_idx = min(start_idx + batch_size, n_samples)
        batch_indices = indices[start_idx:end_idx]
        batches.append(data[batch_indices])

    return batches


def train_autoencoder(model, train_data, optimizer, criterion, epochs, batch_size=64,
                      visualize_every=1, image_shape=(3, 32, 32), model_save_path="fruit_ae_model.pth",
                      log_dir="./runs/fruit_autoencoder"):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    model.to(device)
    model.train()

    writer = SummaryWriter(log_dir=log_dir)

    for epoch in range(epochs):
        batches = create_batches(train_data, batch_size, shuffle=True)

        epoch_loss = 0
        num_batches = len(batches)

        for batch_features in batches:
            batch_features = torch.FloatTensor(batch_features).to(device)

            optimizer.zero_grad()

            outputs = model(batch_features)
            loss = criterion(outputs, batch_features)

            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / num_batches

        writer.add_scalar('Loss', avg_epoch_loss, epoch + 1)

        if visualize_every and (epoch + 1) % visualize_every == 0:
            random_idx = np.random.randint(0, len(train_data))
            viz_sample = train_data[random_idx]
            show_reconstruction(model, viz_sample, epoch + 1, image_shape=image_shape)
            print(f"Epoch [{epoch + 1}/{epochs}], Loss: {avg_epoch_loss:.6f}")


    torch.save(model.state_dict(), model_save_path)
    print(f"Model saved to {model_save_path}")

    writer.close()

import torch
import numpy as np
from torch.utils.tensorboard import SummaryWriter


def create_batches(data, batch_size, shuffle=True):
    n_samples = len(data)
    indices = np.arange(n_samples)

    if shuffle:
        np.random.shuffle(indices)

    batches = []
    for start_idx in range(0, n_samples, batch_size):
        end_idx = min(start_idx + batch_size, n_samples)
        batch_indices = indices[start_idx:end_idx]
        batches.append(data[batch_indices])

    return batches


def train_autoencoder(model, train_data, optimizer, criterion, epochs, batch_size=64,
                      visualize_every=1, image_shape=(3, 32, 32), model_save_path="fruit_ae_model.pth",
                      log_dir="./runs/fruit_ae_2"):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    model.to(device)
    model.train()
    writer = SummaryWriter(log_dir=log_dir)

    for epoch in range(epochs):
        batches = create_batches(train_data, batch_size, shuffle=True)
        epoch_loss = 0
        num_batches = len(batches)

        for batch_features in batches:
            batch_features = torch.FloatTensor(batch_features).to(device)
            optimizer.zero_grad()
            outputs = model(batch_features)
            loss = criterion(outputs, batch_features)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / num_batches
        writer.add_scalar('Loss', avg_epoch_loss, epoch + 1)

        # Fixed visualization call
        if visualize_every and (epoch + 1) % visualize_every == 0:
            # Select multiple random samples for visualization
            random_indices = np.random.choice(len(train_data), min(5, len(train_data)), replace=False)
            viz_samples = [train_data[i] for i in random_indices]
            show_multiple_reconstructions(model, viz_samples, epoch + 1, image_shape=image_shape)
            print(f"Epoch [{epoch + 1}/{epochs}], Loss: {avg_epoch_loss:.6f}")


    torch.save(model.state_dict(), model_save_path)
    print(f"Model saved to {model_save_path}")
    writer.close()

from torch import nn
import torch.optim as optim
import torch
import numpy as np

if __name__ == '__main__':
    # Load fruit dataset
    # UPDATE THIS PATH to point to your fruit dataset folder
    dataset_path = "/root/.cache/kagglehub/datasets/chrisfilo/fruit-recognition/versions/1"  # Change this to your actual dataset path

    print("Loading fruit dataset...")
    images, labels = load_images_from_folder(
        dataset_path,
        target_size=(32, 32),
        max_images_per_class=1000,
        n_classes=5
    )

    # Convert to flattened normalized data
    print("Normalizing images...")
    normalized_data = load_and_normalize_images(images, flatten=True)
    normalized_data = np.array(normalized_data)

    print(f"Dataset shape: {normalized_data.shape}")
    print(f"Number of classes: {len(set(labels))}")
    print(f"Classes: {set(labels)}")

    activation_function = "relu"
    batch_size = 512
    epochs = 900
    lr = 1e-3
    criterion = nn.MSELoss()

    # Use data normalizer if needed (though images are already 0-1 normalized)
    normalizer = DataNormalizer(method=activation_function)
    normalized_data = normalizer.fit_transform(normalized_data)

    # Create AE model for RGB 32x32 images (3072 input dimensions)
    model = ConvAutoencoder(input_shape=3072, activation_function=activation_function)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # Test the model with a sample
    sample = torch.FloatTensor(normalized_data[:1])
    output = model(sample)
    print(f"Before training - Output range: {output.min():.3f} to {output.max():.3f}")
    print(f"Sample input shape: {sample.shape}")
    print(f"Sample output shape: {output.shape}")

    # Train the AE
    train_autoencoder(
        model,
        train_data=normalized_data,
        optimizer=optimizer,
        criterion=criterion,
        epochs=epochs,
        batch_size=batch_size,
        visualize_every=10,
        model_save_path="fruit_ae_model.pth"
    )

    # After training, run comprehensive analysis
    print("\n=== Running Comprehensive Fruit AE Analysis ===")
    latents = comprehensive_fruit_analysis(model, normalized_data, labels, "Fruit Autoencoder")

    # Generate some new samples
    new_samples = generate_samples(model, latents, n_samples=100)
    print(f"\nGenerated {len(new_samples)} new fruit-like images!")

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir runs/fruit_ae_2

def encode_dataset(model, data, batch_size=100):
    """
    Encode dataset with proper error handling and shape checking.
    """
    device = next(model.parameters()).device
    model.eval()

    # Check if data is already encoded (2D latent representations)
    if hasattr(data, 'shape') and len(data.shape) == 2 and data.shape[1] == 2:
        print("Data appears to already be 2D latent representations. Returning as-is.")
        return data

    # Ensure data is the right shape for encoding
    if hasattr(data, 'shape'):
        print(f"Input data shape: {data.shape}")

        # If data is not flattened, flatten it
        if len(data.shape) == 4:  # (N, 3, 32, 32) or (N, 32, 32, 3)
            if data.shape[1] == 3:  # (N, 3, 32, 32)
                data = data.reshape(len(data), -1)
            elif data.shape[3] == 3:  # (N, 32, 32, 3)
                data = data.reshape(len(data), -1)

        # Check if we have the right number of elements
        expected_size = 3 * 32 * 32  # 3072
        if data.shape[1] != expected_size:
            raise ValueError(f"Expected {expected_size} elements per sample, got {data.shape[1]}")

    latents = []

    with torch.no_grad():
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            batch_tensor = torch.FloatTensor(batch).to(device)

            try:
                encoded = model.encode(batch_tensor)
                latents.append(encoded.cpu().numpy())
            except RuntimeError as e:
                print(f"Error encoding batch {i//batch_size + 1}: {e}")
                print(f"Batch shape: {batch_tensor.shape}")
                raise

    model.train()
    return np.vstack(latents)


# Alternative function if your data is already encoded
def use_existing_latents(latent_data, labels, model_name="Fruit AE"):
    """
    Use existing 2D latent representations for analysis.
    """
    print(f"=== {model_name} Analysis with Existing Latents ===\n")

    # Visualize latent space
    print("Visualizing latent space...")
    visualize_latent_space(model, latent_data, labels, f"{model_name} Latent Space")

    # Generate new samples (if model has decode method)
    if hasattr(model, 'decode'):
        print("Generating new samples...")
        show_generated_samples(model, latent_data, n_samples=8)

        # Interpolate between classes
        unique_fruits = list(set(labels))
        if len(unique_fruits) >= 2:
            fruit1, fruit2 = np.random.choice(unique_fruits, 2, replace=False)
            interpolate_between_fruits(model, latent_data, labels, fruit1, fruit2)

    return latent_data

import torch
import matplotlib.pyplot as plt
import numpy as np


def show_reconstruction(model, sample, epoch=None, image_shape=(3, 32, 32), n_samples=5):
    """
    Show reconstructions of n_samples in a single figure.
    Creates a 2xn_samples grid: top row = originals, bottom row = reconstructions
    """
    device = next(model.parameters()).device
    model.eval()

    fig, axes = plt.subplots(2, n_samples, figsize=(15, 6))

    with torch.no_grad():
        for i in range(n_samples):
            # Use the same sample for demonstration (you could modify this to use different samples)
            sample_tensor = torch.FloatTensor(sample).to(device)
            if len(sample_tensor.shape) == 1:
                sample_tensor = sample_tensor.unsqueeze(0)

            # Get reconstruction
            reconstruction = model(sample_tensor)
            reconstruction = reconstruction.cpu().numpy().squeeze()
            original = sample_tensor.cpu().numpy().squeeze()

            # Reshape for display
            original_img = original.reshape(32, 32, 3)
            reconstructed_img = reconstruction.reshape(32, 32, 3)

            # Ensure values are in [0, 1] range for display
            original_img = np.clip(original_img, 0, 1)
            reconstructed_img = np.clip(reconstructed_img, 0, 1)

            # Plot original (top row)
            axes[0, i].imshow(original_img)
            axes[0, i].set_title(f'Original {i + 1}')
            axes[0, i].axis('off')

            # Plot reconstruction (bottom row)
            axes[1, i].imshow(reconstructed_img)
            axes[1, i].set_title(f'Reconstructed {i + 1}')
            axes[1, i].axis('off')

    title = f'Fruit AE Reconstruction - Epoch {epoch}' if epoch else 'Fruit AE Reconstruction'
    plt.suptitle(title, fontsize=16)
    plt.tight_layout()
    plt.show()

    model.train()


def show_multiple_reconstructions(model, data_samples, epoch=None, image_shape=(3, 32, 32), n_samples=5):
    """
    Show reconstructions of multiple different samples.
    data_samples should be a list/array of samples to reconstruct.
    """
    device = next(model.parameters()).device
    model.eval()

    # Select n_samples random samples from the data
    if len(data_samples) >= n_samples:
        selected_indices = np.random.choice(len(data_samples), n_samples, replace=False)
        selected_samples = [data_samples[i] for i in selected_indices]
    else:
        selected_samples = data_samples
        n_samples = len(selected_samples)

    fig, axes = plt.subplots(2, n_samples, figsize=(3 * n_samples, 6))

    # Handle case where n_samples = 1
    if n_samples == 1:
        axes = axes.reshape(2, 1)

    with torch.no_grad():
        for i, sample in enumerate(selected_samples):
            sample_tensor = torch.FloatTensor(sample).to(device)
            if len(sample_tensor.shape) == 1:
                sample_tensor = sample_tensor.unsqueeze(0)

            # Get reconstruction
            reconstruction = model(sample_tensor)
            reconstruction = reconstruction.cpu().numpy().squeeze()
            original = sample_tensor.cpu().numpy().squeeze()

            # Reshape for display
            original_img = original.reshape(32, 32, 3)
            reconstructed_img = reconstruction.reshape(32, 32, 3)

            # Ensure values are in [0, 1] range for display
            original_img = np.clip(original_img, 0, 1)
            reconstructed_img = np.clip(reconstructed_img, 0, 1)

            # Plot original (top row)
            axes[0, i].imshow(original_img)
            axes[0, i].set_title(f'Original {i + 1}')
            axes[0, i].axis('off')

            # Plot reconstruction (bottom row)
            axes[1, i].imshow(reconstructed_img)
            axes[1, i].set_title(f'Reconstructed {i + 1}')
            axes[1, i].axis('off')

    title = f'Fruit AE Reconstructions - Epoch {epoch}' if epoch else 'Fruit AE Reconstructions'
    plt.suptitle(title, fontsize=16)
    plt.tight_layout()
    plt.show()

    model.train()


def visualize_latent_space(model, data, labels=None, title="Fruit AE Latent Space"):
    """
    Visualize the 2D latent space with fruit classes color-coded.
    """

    # Get latent representations
    if hasattr(data, 'shape') and len(data.shape) == 2:
        latent_representations = encode_dataset(model, data)
    else:
        latent_representations = data  # Assume it's already encoded

    plt.figure(figsize=(12, 8))

    if labels is not None:
        unique_classes = np.unique(labels)
        n_classes = len(unique_classes)

        # Use distinct colors for each fruit class
        colors = plt.cm.Set3(np.linspace(0, 1, n_classes))

        for i, class_label in enumerate(unique_classes):
            mask = np.array(labels) == class_label
            plt.scatter(latent_representations[mask, 0], latent_representations[mask, 1],
                        c=[colors[i]], label=f'{class_label}', s=10, alpha=0.7)

        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.scatter(latent_representations[:, 0], latent_representations[:, 1],
                    s=10, alpha=0.7, c='blue')

    plt.xlabel('Latent Dimension 1')
    plt.ylabel('Latent Dimension 2')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()


def show_sample_images(images, labels=None, title="Sample Fruit Images", n=8):
    """
    Display sample fruit images from the dataset.
    """
    if hasattr(images, 'numpy'):
        images = images.numpy()

    fig, axes = plt.subplots(2, 4, figsize=(12, 6))
    fig.suptitle(title, fontsize=16, fontweight='bold')

    for i, ax in enumerate(axes.flat):
        if i < len(images) and i < n:
            img = images[i].reshape(32, 32, 3)
            img = np.clip(img, 0, 1)  # Ensure values are in [0, 1]
            ax.imshow(img)
            if labels is not None:
                ax.set_title(f'{labels[i]}')
            else:
                ax.set_title(f'Sample {i + 1}')
        ax.axis('off')

    plt.tight_layout()
    plt.show()


def show_generated_samples(model, latent_representations, n_samples=8, title="Generated Fruit Images"):
    """
    Generate and display new fruit images by sampling from the latent space.
    """

    generated_images = generate_samples(model, latent_representations, n_samples=n_samples)

    fig, axes = plt.subplots(2, 4, figsize=(12, 6))
    fig.suptitle(title, fontsize=16, fontweight='bold')

    for i, ax in enumerate(axes.flat):
        if i < n_samples:
            img = generated_images[i].reshape(32, 32, 3)
            img = np.clip(img, 0, 1)
            ax.imshow(img)
            ax.set_title(f'Generated #{i + 1}')
        ax.axis('off')

    plt.tight_layout()
    plt.show()


def interpolate_between_fruits(model, latent_representations, labels, fruit1, fruit2, n_steps=8):
    """
    Interpolate between two different fruit classes in the latent space.
    """
    device = next(model.parameters()).device
    model.eval()

    # Find indices for each fruit class
    fruit1_indices = np.where(np.array(labels) == fruit1)[0]
    fruit2_indices = np.where(np.array(labels) == fruit2)[0]

    if len(fruit1_indices) == 0 or len(fruit2_indices) == 0:
        print(f"Could not find examples of {fruit1} or {fruit2}")
        return

    # Use random examples from each class
    start_idx = np.random.choice(fruit1_indices)
    end_idx = np.random.choice(fruit2_indices)

    start_latent = latent_representations[start_idx]
    end_latent = latent_representations[end_idx]

    # Create interpolation steps
    interpolation_steps = np.linspace(0, 1, n_steps)
    interpolated_latents = []

    for step in interpolation_steps:
        interpolated = start_latent * (1 - step) + end_latent * step
        interpolated_latents.append(interpolated)

    interpolated_latents = np.array(interpolated_latents)

    # Generate images from interpolated latents
    with torch.no_grad():
        latent_tensor = torch.FloatTensor(interpolated_latents).to(device)
        interpolated_images = model.decode(latent_tensor).cpu().numpy()

    # Display the interpolation
    fig, axes = plt.subplots(1, n_steps, figsize=(2 * n_steps, 3))
    fig.suptitle(f'Fruit AE Interpolation: {fruit1} â†’ {fruit2}', fontsize=16, fontweight='bold')

    for i in range(n_steps):
        img = interpolated_images[i].reshape(32, 32, 3)
        img = np.clip(img, 0, 1)
        axes[i].imshow(img)
        axes[i].set_title(f'Step {i + 1}')
        axes[i].axis('off')

    plt.tight_layout()
    plt.show()
    model.train()


def explore_latent_space_grid(model, latent_representations, grid_size=5, scale=2.0):
    """
    Explore the latent space by generating images on a regular grid.
    """
    device = next(model.parameters()).device
    model.eval()

    # Calculate latent space statistics
    latent_mean = np.mean(latent_representations, axis=0)
    latent_std = np.std(latent_representations, axis=0)

    # Create grid points
    x_range = np.linspace(-scale * latent_std[0], scale * latent_std[0], grid_size)
    y_range = np.linspace(-scale * latent_std[1], scale * latent_std[1], grid_size)

    grid_latents = []
    for y in y_range:
        for x in x_range:
            latent_point = latent_mean.copy()
            latent_point[0] = latent_mean[0] + x
            latent_point[1] = latent_mean[1] + y
            grid_latents.append(latent_point)

    grid_latents = np.array(grid_latents)

    # Generate images from grid points
    with torch.no_grad():
        latent_tensor = torch.FloatTensor(grid_latents).to(device)
        generated_images = model.decode(latent_tensor).cpu().numpy()

    # Display the grid
    fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))
    fig.suptitle('Fruit AE Latent Space Grid Exploration', fontsize=16, fontweight='bold')

    for i in range(grid_size):
        for j in range(grid_size):
            idx = i * grid_size + j
            img = generated_images[idx].reshape(32, 32, 3)
            img = np.clip(img, 0, 1)
            axes[i, j].imshow(img)
            axes[i, j].axis('off')

    plt.tight_layout()
    plt.show()
    model.train()


def perturb_existing_samples(model, data_samples, labels, latent_representations, n_samples=8, noise_scale=0.5):
    """
    Show original samples vs their perturbed versions in latent space.
    """
    device = next(model.parameters()).device
    model.eval()

    # Select random samples
    indices = np.random.choice(len(latent_representations), n_samples, replace=False)

    fig, axes = plt.subplots(2, n_samples, figsize=(2 * n_samples, 6))
    fig.suptitle('Original vs Perturbed Fruit Images', fontsize=16, fontweight='bold')

    for i, idx in enumerate(indices):
        # Show original
        original_img = data_samples[idx].reshape(32, 32, 3)
        original_img = np.clip(original_img, 0, 1)
        axes[0, i].imshow(original_img)
        axes[0, i].set_title(f'Original: {labels[idx]}')
        axes[0, i].axis('off')

        # Generate perturbed version
        original_latent = latent_representations[idx]
        noise = np.random.normal(0, noise_scale, original_latent.shape)
        perturbed_latent = original_latent + noise

        with torch.no_grad():
            latent_tensor = torch.FloatTensor(perturbed_latent).unsqueeze(0).to(device)
            generated_image = model.decode(latent_tensor).cpu().numpy().squeeze()

        perturbed_img = generated_image.reshape(32, 32, 3)
        perturbed_img = np.clip(perturbed_img, 0, 1)
        axes[1, i].imshow(perturbed_img)
        axes[1, i].set_title('Perturbed')
        axes[1, i].axis('off')

    plt.tight_layout()
    plt.show()
    model.train()


def comprehensive_fruit_analysis(model, data, labels, model_name="Fruit AE"):
    """
    Run a comprehensive analysis of the trained fruit AE.
    """

    print(f"=== {model_name} Comprehensive Analysis ===\n")

    # 1. Show sample original images
    print("1. Showing sample original fruit images...")
    show_sample_images(data[:8], labels[:8], "Original Fruit Dataset Samples")

    # 2. Show reconstructions
    print("2. Showing sample reconstructions...")
    random_indices = np.random.choice(len(data), 5, replace=False)
    sample_data = [data[i] for i in random_indices]
    show_multiple_reconstructions(model, sample_data, image_shape=(3, 32, 32))

    # 3. Encode to latent space
    print("3. Encoding dataset to latent space...")
    latents = encode_dataset(model, data)

    # 4. Visualize latent space
    print("4. Visualizing latent space...")
    visualize_latent_space(model, latents, labels, f"{model_name} Latent Space")

    # 5. Generate new samples
    print("5. Generating new fruit images...")
    show_generated_samples(model, latents, n_samples=8)

    # 6. Interpolate between fruit classes
    print("6. Interpolating between different fruit classes...")
    unique_fruits = list(set(labels))
    if len(unique_fruits) >= 2:
        fruit1, fruit2 = np.random.choice(unique_fruits, 2, replace=False)
        interpolate_between_fruits(model, latents, labels, fruit1, fruit2)

    # 7. Explore latent space systematically
    print("7. Exploring latent space grid...")
    explore_latent_space_grid(model, latents, grid_size=5)

    # 8. Perturb existing samples
    print("8. Showing perturbations of existing samples...")
    perturb_existing_samples(model, data, labels, latents, n_samples=6, noise_scale=0.3)

    print(f"\n=== {model_name} Analysis Complete! ===")

    return latents

latents = comprehensive_fruit_analysis(model, normalized_data, labels, "Fruit Autoencoder")

